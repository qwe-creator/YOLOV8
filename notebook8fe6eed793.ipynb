{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":4931060,"sourceType":"datasetVersion","datasetId":2859502},{"sourceId":12120869,"sourceType":"datasetVersion","datasetId":7632077}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":3906.378257,"end_time":"2025-06-08T22:06:12.829173","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-06-08T21:01:06.450916","version":"2.6.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"fba070dd","cell_type":"code","source":"import yaml # Used for reading and writing YAML configuration files.\nimport os # Provides functions for interacting with the operating system, like file paths and directory creation.\nimport json # Used for handling JSON data, specifically for COCO annotation files.\nfrom tqdm import tqdm # Provides progress bars, useful for visualizing the progress of loops (e.g., dataset conversion).\nfrom PIL import Image # Pillow library, used for opening and manipulating image files to get their dimensions.\nimport shutil # Provides high-level file operations, like copying files.\nimport time # Used for time-related functions, specifically for pausing execution (e.g., waiting for training to complete).\nimport torch # PyTorch library, essential for deep learning operations, especially for clearing CUDA cache and loading/saving model checkpoints.\nimport sys # Provides access to system-specific parameters and functions, though not directly used for system exit in this version.\n\n# This command changes the current working directory to '/kaggle/working/'.\n# In Kaggle notebooks, files saved here persist across sessions. It's good practice\n# to operate within this directory for output files and cloned repositories.\n%cd /kaggle/working/\n\nprint(\"Cleaning up previous NanoDet installation...\")\n# This command removes the 'nanodet' directory and its contents if it exists.\n# This ensures a clean slate for cloning, preventing potential conflicts from previous,\n# incomplete or altered installations.\n!rm -rf nanodet\n\nprint(\"Cloning NanoDet repository...\")\n# This command clones the official NanoDet repository from GitHub.\n# It downloads all the source code, scripts, and default configurations\n# needed to run NanoDet models.\n!git clone https://github.com/RangiLyu/nanodet.git\n\n# This command changes the current working directory into the newly cloned 'nanodet/' folder.\n# All subsequent commands related to NanoDet's internal scripts (like 'train.py', 'test.py')\n# or configuration files will be executed relative to this directory.\n%cd nanodet/\n\nprint(\"\\nVerifying NanoDet directory structure:\")\n# Lists the contents of the current directory (which is now '/kaggle/working/nanodet/').\n# This helps confirm that the cloning was successful and the main folders are present.\n!ls -F\n# Lists the contents of the 'config/' subdirectory. This shows the available\n# NanoDet configuration templates.\n!ls -F config/\n# Lists the contents of the 'config/legacy_v0.x_configs/' subdirectory.\n# This is specifically checked because the chosen base configuration for this project\n# is located within this legacy folder for stability.\n!ls -F config/legacy_v0.x_configs/","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-06-13T04:44:33.006411Z","iopub.execute_input":"2025-06-13T04:44:33.007082Z","iopub.status.idle":"2025-06-13T04:44:34.259300Z","shell.execute_reply.started":"2025-06-13T04:44:33.007056Z","shell.execute_reply":"2025-06-13T04:44:34.258599Z"},"papermill":{"duration":5.801359,"end_time":"2025-06-08T21:01:16.484988","exception":false,"start_time":"2025-06-08T21:01:10.683629","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/working\nCleaning up previous NanoDet installation...\nCloning NanoDet repository...\nCloning into 'nanodet'...\nremote: Enumerating objects: 2722, done.\u001b[K\nremote: Total 2722 (delta 0), reused 0 (delta 0), pack-reused 2722 (from 1)\u001b[K\nReceiving objects: 100% (2722/2722), 5.29 MiB | 32.84 MiB/s, done.\nResolving deltas: 100% (1602/1602), done.\n/kaggle/working/nanodet\n\nVerifying NanoDet directory structure:\nconfig/\t\t    demo_libtorch/  demo_openvino/  nanodet/\t      setup.py\ndemo/\t\t    demo_mnn/\t    docs/\t    README.md\t      tests/\ndemo_android_ncnn/  demo_ncnn/\t    LICENSE\t    requirements.txt  tools/\nconvnext/\t\t\tnanodet-plus-m-1.5x_416.yml\nlegacy_v0.x_configs/\t\tnanodet-plus-m_320.yml\nnanodet_custom_xml_dataset.yml\tnanodet-plus-m_416.yml\nnanodet-plus-m-1.5x_320.yml\tnanodet-plus-m_416-yolo.yml\nEfficientNet-Lite/  nanodet-m-1.5x-416.yml  nanodet-m.yml\nnanodet-g.yml\t    nanodet-m-1.5x.yml\t    RepVGG/\nnanodet-m-0.5x.yml  nanodet-m-416.yml\t    Transformer/\n","output_type":"stream"}],"execution_count":16},{"id":"4cd63cb5","cell_type":"code","source":"# --- APPLY PATCHES HERE ---\n# These patches are critical fixes to ensure the NanoDet codebase functions correctly\n# within modern Python environments (like those on Kaggle) and with newer PyTorch versions.\n# NanoDet might have been originally developed with older dependencies, causing compatibility issues.\n\nprint(\"\\nApplying patch for torch._six compatibility...\")\n# This 'sed' (stream editor) command modifies the file 'nanodet/nanodet/data/collate.py'.\n# The original line 19 attempts to import 'string_classes' from 'torch._six'.\n# However, 'torch._six' has been deprecated and removed in recent PyTorch versions.\n# The patch replaces this import with 'string_classes = str', directly defining\n# 'string_classes' as the standard Python string type, resolving the import error.\n!sed -i \"19s/from torch._six import string_classes/string_classes = str/\" /kaggle/working/nanodet/nanodet/data/collate.py\nprint(\"✅ Torch._six patch applied successfully!\")\n\nprint(\"\\nApplying patch for PyTorch Lightning strategy...\")\n# This 'sed' command modifies the file 'nanodet/tools/train.py'.\n# In newer versions of PyTorch Lightning (which NanoDet uses), the 'strategy' argument\n# for the Trainer might expect a string ('auto', 'ddp', etc.) rather than a direct object.\n# The original code might be passing a strategy object directly. This patch changes\n# line 146 to set `strategy='auto'`, allowing PyTorch Lightning to automatically\n# select the appropriate distributed training strategy for the environment, preventing errors.\n!sed -i \"146s/strategy=strategy,/strategy='auto',/\" /kaggle/working/nanodet/tools/train.py\nprint(\"✅ PyTorch Lightning strategy patch applied successfully!\")\n\nprint(\"\\nApplying patch for test.py to bypass checkpoint conversion...\")\n# This 'sed' command modifies the file 'nanodet/tools/test.py'.\n# NanoDet includes a utility function `convert_old_model` to update old checkpoint formats\n# to be compatible with newer model architectures. However, for models newly trained\n# within the current setup, this conversion is often unnecessary and can sometimes\n# cause errors if the checkpoint format is already compatible. This patch comments out\n# the line (line 83) that calls `convert_old_model`, effectively bypassing this step.\n!sed -i \"83s/ckpt = convert_old_model(ckpt)/# ckpt = convert_old_model(ckpt) # Patched to bypass conversion/\" /kaggle/working/nanodet/tools/test.py\nprint(\"✅ test.py checkpoint conversion bypass patch applied successfully!\")","metadata":{"execution":{"iopub.status.busy":"2025-06-13T04:44:38.977133Z","iopub.execute_input":"2025-06-13T04:44:38.977773Z","iopub.status.idle":"2025-06-13T04:44:39.351063Z","shell.execute_reply.started":"2025-06-13T04:44:38.977743Z","shell.execute_reply":"2025-06-13T04:44:39.350357Z"},"papermill":{"duration":0.371816,"end_time":"2025-06-08T21:01:16.860518","exception":false,"start_time":"2025-06-08T21:01:16.488702","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"\nApplying patch for torch._six compatibility...\n✅ Torch._six patch applied successfully!\n\nApplying patch for PyTorch Lightning strategy...\n✅ PyTorch Lightning strategy patch applied successfully!\n\nApplying patch for test.py to bypass checkpoint conversion...\n✅ test.py checkpoint conversion bypass patch applied successfully!\n","output_type":"stream"}],"execution_count":17},{"id":"7699bac6","cell_type":"code","source":"print(\"\\nInstalling dependencies...\")\n# Install required Python packages silently (the '-q' flag).\n# 1. `pyyaml`: Necessary for parsing and generating YAML files, which are used for NanoDet configurations.\n# 2. `opencv-python`: The Python bindings for OpenCV, widely used for image and video processing,\n#    including operations like image loading, resizing, and augmentations in the data pipeline.\n# 3. `tqdm`: A library for creating fast, extensible progress bars, making long processes (like dataset conversion)\n#    more user-friendly by showing progress.\n# 4. `tensorboard`: Google's visualization toolkit for machine learning. It's used by NanoDet to log\n#    training metrics (loss, mAP, etc.) which can then be visualized in a web interface.\n# 5. `torchmetrics`: A collection of PyTorch-specific metrics implementations. Used by NanoDet for evaluation\n#    metrics like mean Average Precision (mAP).\n# 6. `pycocotools`: Python API for the COCO (Common Objects in Context) dataset. This library is essential\n#    for working with COCO-formatted annotations and calculating COCO evaluation metrics (like mAP).\n!pip install -q pyyaml opencv-python tqdm tensorboard torchmetrics pycocotools\n# Install NanoDet itself in 'editable' mode (`-e .`). The '.' refers to the current directory\n# (which is '/kaggle/working/nanodet/'). Editable mode means that Python installs the package\n# by creating a link to the source directory. This is beneficial because any changes\n# made directly to the NanoDet source files (like the patches applied above) will\n# immediately take effect without needing to reinstall the package.\n!pip install -q -e .\n\nprint(\"\\n\\n✅ Environment setup and installation complete!\")","metadata":{"execution":{"iopub.status.busy":"2025-06-13T04:44:43.701898Z","iopub.execute_input":"2025-06-13T04:44:43.702190Z","iopub.status.idle":"2025-06-13T04:44:52.403164Z","shell.execute_reply.started":"2025-06-13T04:44:43.702166Z","shell.execute_reply":"2025-06-13T04:44:52.402104Z"},"papermill":{"duration":82.487954,"end_time":"2025-06-08T21:02:39.352040","exception":false,"start_time":"2025-06-08T21:01:16.864086","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"\nInstalling dependencies...\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\n\n✅ Environment setup and installation complete!\n","output_type":"stream"}],"execution_count":18},{"id":"4a3419a0","cell_type":"code","source":"# --- 1. Dataset Conversion (YOLO to COCO) ---\n# NanoDet models are designed to work with the COCO dataset format. The HIT-UAV dataset,\n# however, is provided in the YOLO (You Only Look Once) format. Therefore, this step\n# is crucial to convert the dataset into a format compatible with NanoDet's training and evaluation pipelines.\n\ndef convert_yolo_to_coco(dataset_root_path, output_dir):\n    \"\"\"\n    Converts a YOLO-formatted dataset (images and text labels) into the COCO JSON format.\n    It processes 'train', 'val', and 'test' splits, extracting bounding box information,\n    and copies the images to a new COCO-style directory structure.\n\n    Args:\n        dataset_root_path (str): The root directory of the YOLO-formatted dataset.\n                                 Expected structure: `dataset_root_path/images/{split}/` and\n                                 `dataset_root_path/labels/{split}/`.\n        output_dir (str): The directory where the COCO-formatted data (images and JSONs)\n                          will be saved.\n    \"\"\"\n    print(\"Starting dataset conversion from YOLO to COCO format...\")\n    # Create the necessary subdirectories in the `output_dir` for organizing the\n    # COCO-formatted dataset.\n    # 'annotations': Will store the COCO JSON files (train.json, val.json, test.json).\n    # 'train', 'val', 'test': Will store the images for each corresponding split.\n    os.makedirs(os.path.join(output_dir, 'annotations'), exist_ok=True)\n    os.makedirs(os.path.join(output_dir, 'train'), exist_ok=True)\n    os.makedirs(os.path.join(output_dir, 'val'), exist_ok=True)\n    os.makedirs(os.path.join(output_dir, 'test'), exist_ok=True) # Ensure 'test' directory is created\n    \n    # Construct the full path to the `dataset.yaml` file. This file typically contains\n    # metadata about the dataset, most importantly the names of the classes.\n    # It's confirmed that `dataset.yaml` is located directly under the `hit-uav` folder.\n    yaml_path = os.path.join(dataset_root_path, 'dataset.yaml')\n    \n    print(f\"Attempting to open dataset.yaml at: {yaml_path}\") # Debugging print statement for path verification\n\n    # Open and parse the `dataset.yaml` file to extract the class names.\n    with open(yaml_path, 'r') as f:\n        data_yaml = yaml.safe_load(f)\n    class_names = data_yaml['names']\n    print(f\"Found {len(class_names)} classes: {class_names}\")\n\n    # Iterate through each dataset split to perform the conversion individually.\n    # The HIT-UAV dataset has 'train', 'val', and 'test' splits, which are all processed.\n    for split in ['train', 'val', 'test']:\n        print(f\"\\nProcessing '{split}' set...\")\n        \n        # Initialize the Python dictionary that will hold the COCO JSON structure for the current split.\n        # This dictionary includes:\n        # - \"info\": General dataset information (can be left empty for this purpose).\n        # - \"licenses\": Licensing information (can be left empty).\n        # - \"categories\": A list of dictionaries, where each dictionary defines a class with an ID, name, and supercategory.\n        #   The IDs are assigned sequentially starting from 0, matching the YOLO class IDs.\n        # - \"images\": A list of dictionaries, each representing an image with its ID, filename, width, and height.\n        # - \"annotations\": A list of dictionaries, each representing a single bounding box annotation.\n        coco_output = {\n            \"info\": {},\n            \"licenses\": [],\n            \"categories\": [{\"id\": i, \"name\": name, \"supercategory\": \"object\"} for i, name in enumerate(class_names)],\n            \"images\": [],\n            \"annotations\": []\n        }\n        \n        # Construct the paths to the image and label directories for the current split.\n        image_dir = os.path.join(dataset_root_path, 'images', split)\n        label_dir = os.path.join(dataset_root_path, 'labels', split)\n        \n        # Before processing, check if the image directory for the current split actually exists\n        # and contains any files. If not, print a warning and skip this split to avoid errors.\n        if not os.path.exists(image_dir) or not os.listdir(image_dir):\n            print(f\"Warning: No images found for '{split}' split at {image_dir}. Skipping conversion for this split.\")\n            continue # Move to the next split in the loop.\n\n        # Get a sorted list of all image files (JPG, JPEG, PNG) within the current split's image directory.\n        image_files = sorted([f for f in os.listdir(image_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n        \n        # Initialize counters for assigning unique IDs to images and annotations in the COCO JSON.\n        image_id_counter = 0\n        annotation_id_counter = 0\n\n        # Loop through each image file to process its information and convert its annotations.\n        for image_file in tqdm(image_files): # `tqdm` wraps the iterator to display a progress bar.\n            image_path = os.path.join(image_dir, image_file)\n            \n            # Open the image using Pillow to retrieve its dimensions (width and height).\n            # These dimensions are crucial for converting normalized YOLO coordinates to absolute pixel values for COCO.\n            with Image.open(image_path) as img:\n                img_width, img_height = img.size\n            \n            # Create a dictionary for the current image's metadata and add it to the \"images\" list in `coco_output`.\n            image_info = {\"id\": image_id_counter, \"file_name\": image_file, \"width\": img_width, \"height\": img_height}\n            coco_output[\"images\"].append(image_info)\n            \n            # Construct the path to the corresponding YOLO label file (.txt).\n            # YOLO label files typically have the same base name as the image but with a '.txt' extension.\n            label_file = os.path.splitext(image_file)[0] + '.txt'\n            label_path = os.path.join(label_dir, label_file)\n\n            # Check if a corresponding label file exists for the current image.\n            if os.path.exists(label_path):\n                # If the label file exists, open and read each line. Each line represents one object detection.\n                with open(label_path, 'r') as f:\n                    for line in f:\n                        # Parse the YOLO annotation format: `class_id x_center y_center width height`.\n                        # All values are typically normalized (0 to 1) relative to image dimensions.\n                        class_id, x_center, y_center, width, height = map(float, line.strip().split())\n                        \n                        # Convert normalized YOLO coordinates to absolute COCO bounding box format.\n                        # COCO bounding box format: `[x_top_left, y_top_left, bbox_width, bbox_height]`.\n                        x_min = (x_center - width / 2) * img_width # Calculate x-coordinate of the top-left corner.\n                        y_min = (y_center - height / 2) * img_height # Calculate y-coordinate of the top-left corner.\n                        bbox_width = width * img_width # Calculate absolute width of the bounding box.\n                        bbox_height = height * img_height # Calculate absolute height of the bounding box.\n                        \n                        # Create an annotation dictionary for the current object detection and add it to the \"annotations\" list.\n                        annotation_info = {\n                            \"id\": annotation_id_counter,        # Unique ID for this annotation.\n                            \"image_id\": image_id_counter,      # ID of the image this annotation belongs to.\n                            \"category_id\": int(class_id),      # The integer ID of the detected class.\n                            \"bbox\": [x_min, y_min, bbox_width, bbox_height], # The bounding box coordinates in COCO format.\n                            \"area\": bbox_width * bbox_height, # The area of the bounding box.\n                            \"iscrowd\": 0 # Indicates if the object is a crowd (0 for individual objects).\n                        }\n                        coco_output[\"annotations\"].append(annotation_info)\n                        annotation_id_counter += 1 # Increment annotation ID for the next object.\n            \n            # After processing annotations, copy the current image file to its respective COCO-formatted\n            # output directory (e.g., `hituav_coco/train/`).\n            shutil.copy(image_path, os.path.join(output_dir, split, image_file))\n            image_id_counter += 1 # Increment image ID for the next image file.\n\n        # After processing all images and labels for a specific split, save the `coco_output`\n        # dictionary as a JSON file in the 'annotations' subdirectory.\n        output_json_path = os.path.join(output_dir, 'annotations', f'{split}.json')\n        with open(output_json_path, 'w') as f:\n            json.dump(coco_output, f) # `json.dump` writes the Python dictionary to a JSON formatted file.\n        print(f\"Successfully created COCO annotation file: {output_json_path}\")","metadata":{"execution":{"iopub.status.busy":"2025-06-13T04:45:02.323848Z","iopub.execute_input":"2025-06-13T04:45:02.324144Z","iopub.status.idle":"2025-06-13T04:45:02.338422Z","shell.execute_reply.started":"2025-06-13T04:45:02.324119Z","shell.execute_reply":"2025-06-13T04:45:02.337859Z"},"papermill":{"duration":0.038347,"end_time":"2025-06-08T21:02:39.411533","exception":false,"start_time":"2025-06-08T21:02:39.373186","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":19},{"id":"ae5e4764","cell_type":"code","source":"# --- Run the conversion process ---\n# Define the input path for the original YOLO-formatted HIT-UAV dataset\n# and the output path where the COCO-formatted dataset will be stored.\nyolo_dataset_path = '/kaggle/input/yepppp/hit-uav'\ncoco_output_path = '/kaggle/working/hituav_coco'\n\n# Call the `convert_yolo_to_coco` function to start the conversion.\nconvert_yolo_to_coco(yolo_dataset_path, coco_output_path)\nprint(\"\\n\\n✅ Dataset conversion complete!\")","metadata":{"execution":{"iopub.status.busy":"2025-06-13T04:45:09.502251Z","iopub.execute_input":"2025-06-13T04:45:09.502517Z","iopub.status.idle":"2025-06-13T04:45:22.783395Z","shell.execute_reply.started":"2025-06-13T04:45:09.502498Z","shell.execute_reply":"2025-06-13T04:45:22.782859Z"},"papermill":{"duration":47.190809,"end_time":"2025-06-08T21:03:26.623578","exception":false,"start_time":"2025-06-08T21:02:39.432769","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Starting dataset conversion from YOLO to COCO format...\nAttempting to open dataset.yaml at: /kaggle/input/yepppp/hit-uav/dataset.yaml\nFound 5 classes: {0: 'Person', 1: 'Car', 2: 'Bicycle', 3: 'OtherVehicle', 4: 'DontCare'}\n\nProcessing 'train' set...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2008/2008 [00:09<00:00, 221.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Successfully created COCO annotation file: /kaggle/working/hituav_coco/annotations/train.json\n\nProcessing 'val' set...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 287/287 [00:01<00:00, 221.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"Successfully created COCO annotation file: /kaggle/working/hituav_coco/annotations/val.json\n\nProcessing 'test' set...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 571/571 [00:02<00:00, 219.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Successfully created COCO annotation file: /kaggle/working/hituav_coco/annotations/test.json\n\n\n✅ Dataset conversion complete!\n","output_type":"stream"}],"execution_count":20},{"id":"702fe7d6","cell_type":"code","source":"# --- 2. Configuration Modification for NanoDet (Using Legacy Template) ---\n# This section focuses on adapting NanoDet's default configuration to train\n# on the specific HIT-UAV dataset. It involves setting correct data paths,\n# defining class information, and fine-tuning training parameters.\n\n# Define the path to the base configuration template.\n# 'config/legacy_v0.x_configs/nanodet-m.yml' is chosen for its compatibility\n# and stable performance with older NanoDet versions, as suggested by the user.\nconfig_template_path = 'config/legacy_v0.x_configs/nanodet-m.yml'\n# Define the path for the new custom configuration file. This file will be created\n# by copying the template and then modified.\ncustom_config_path = 'config/nanodet_hituav.yml'\n\n# Copy the base configuration template to the new custom config file.\n# This ensures that any modifications are made to a separate file,\n# preserving the original template for future use.\n!cp {config_template_path} {custom_config_path}\n\nprint(f\"Pivoting to stable legacy config: {config_template_path}\")\nprint(f\"Loading configuration file: {custom_config_path}\")\n\n# Load the copied custom YAML configuration file into a Python dictionary.\n# This allows programmatic access and modification of the configuration parameters.\nwith open(custom_config_path, 'r') as f:\n    config = yaml.safe_load(f)\n\n# --- Making all the necessary changes for the legacy model configuration ---\n\n# Set the directory where model checkpoints (e.g., 'model_best.pth', 'latest.pth')\n# and training logs will be saved. It's set to a persistent location in Kaggle.\nconfig['save_dir'] = '/kaggle/working/model_workspace/'\n\n# Define the list of class names that the model will learn to detect.\n# This list must precisely match the class names and their order (indexed from 0)\n# as defined in your dataset's `dataset.yaml` file and used during COCO conversion.\n# The HIT-UAV dataset has 5 classes, including 'DontCare'.\nconfig['class_names'] = [\"Person\", \"Car\", \"Bicycle\", \"OtherVehicle\", \"DontCare\"]\n# Set the number of output classes for the model's detection head.\n# This value must directly correspond to the total number of classes in `class_names`.\nconfig['model']['arch']['head']['num_classes'] = 5\n\n# Update the image and annotation file paths for the training data loader.\n# These paths point to the COCO-formatted dataset generated in Step 1.\nconfig['data']['train']['img_path'] = '/kaggle/working/hituav_coco/train'\nconfig['data']['train']['ann_path'] = '/kaggle/working/hituav_coco/annotations/train.json'\n# Update the image and annotation file paths for the validation data loader.\n# These paths also point to the COCO-formatted dataset.\nconfig['data']['val']['img_path'] = '/kaggle/working/hituav_coco/val'\nconfig['data']['val']['ann_path'] = '/kaggle/working/hituav_coco/annotations/val.json'\n\n# Add the configuration specifically for the test data split.\n# While the training and validation paths were updated in existing 'data' sections,\n# 'test' might need to be explicitly defined or fully overridden.\nconfig['data']['test'] = {\n    'name': 'CocoDataset', # Specifies that this is a COCO-formatted dataset.\n    'img_path': '/kaggle/working/hituav_coco/test', # Path to the test images.\n    'ann_path': '/kaggle/working/hituav_coco/annotations/test.json', # Path to the test annotations JSON file.\n    'input_size': [320, 320], # The input resolution (width, height) for the model during testing.\n                              # It's set to 320x320 for consistency with the 'nanodet-m' legacy model.\n    'keep_ratio': True, # A common setting to maintain the aspect ratio of images during resizing.\n    'pipeline': { # Defines the data augmentation and preprocessing pipeline for the test set.\n                  # Test pipelines are typically simpler than training pipelines.\n        'perspective': 0.0, 'scale': [0.6, 1.4], 'stretch': [0.8, 1.2], # Geometric augmentations (disabled or scaled down for test)\n        'rot_angle': 0.0, 'shear': 0.0, 'translate': 0.2, 'mosaic': 1.0,\n        'mixup': 0.15, 'cutmix': 0.0, 'hsv': 0.015, 'flip': 0.5, # Color jittering and flipping (often enabled for test)\n        'degrees': 0.0, 'image_max_range': [0, 255] # Other transformations\n    }\n}\n\n# Set a consistent input size for both training and validation data loaders as well.\n# This ensures that all data loaders (train, val, test) use the same input resolution.\nconfig['data']['train']['input_size'] = [320, 320]\nconfig['data']['val']['input_size'] = [320, 320]\n\n# Ensure 'dataloader_cfg' exists within the 'data' section and set the number of worker processes.\n# 'num_workers' determines how many subprocesses Python's DataLoader will use to load data.\n# A higher number can speed up data loading by pre-fetching, but it consumes more CPU and RAM.\n# 4 is a common, balanced choice, and often matches the number of CPU cores allocated in Kaggle for GPU kernels.\nif 'dataloader_cfg' not in config['data']:\n    config['data']['dataloader_cfg'] = {}\nconfig['data']['dataloader_cfg']['num_workers'] = 4\n\n# Update device and schedule settings for training optimization.\n# `batch_size_per_gpu`: The number of images processed in parallel on a single GPU.\n# This is a critical parameter that directly impacts GPU memory usage. A value of 4 is chosen\n# to balance performance and memory consumption. If you encounter Out-Of-Memory (OOM) errors,\n# you should reduce this value (e.g., to 2 or 1).\nconfig['device']['batch_size_per_gpu'] = 4\n# `accumulate_grad_batches`: This parameter enables gradient accumulation. The model\n# calculates gradients for this many batches before performing a single optimizer step\n# (i.e., updating model weights). This effectively simulates a larger batch size\n# without requiring more GPU memory to hold multiple batches simultaneously.\n# Here, an effective batch size of 4 (batch_size_per_gpu) * 2 (accumulate_grad_batches) = 8 is achieved.\nconfig['schedule']['accumulate_grad_batches'] = 2\n\n# Set the total number of training epochs. An epoch represents one full pass over the entire training dataset.\n# A higher number of epochs generally leads to better model performance but requires more training time.\n# 200 epochs is a common setting for object detection tasks.\nconfig['schedule']['total_epochs'] = 20\n# Set the number of warmup epochs. During the warmup phase, the learning rate\n# gradually increases from a very small value to its full, configured value.\n# This helps stabilize training at the beginning, especially with large batch sizes.\nconfig['schedule']['warmup']['warmup_epochs'] = 5\n\n# Write the updated configuration dictionary back to the custom YAML file.\n# `sort_keys=False`: Preserves the original order of keys in the YAML file for better readability.\n# `default_flow_style=False`: Uses the \"block style\" for YAML output, which is more human-readable\n# than the \"flow style\" (which puts everything on one line).\nwith open(custom_config_path, 'w') as f:\n    yaml.dump(config, f, sort_keys=False, default_flow_style=False)\n\nprint(f\"\\n✅ Successfully modified and saved {custom_config_path}. Ready for training!\")","metadata":{"execution":{"iopub.status.busy":"2025-06-13T04:45:39.426573Z","iopub.execute_input":"2025-06-13T04:45:39.427344Z","iopub.status.idle":"2025-06-13T04:45:39.577586Z","shell.execute_reply.started":"2025-06-13T04:45:39.427318Z","shell.execute_reply":"2025-06-13T04:45:39.576854Z"},"papermill":{"duration":0.187983,"end_time":"2025-06-08T21:03:26.849445","exception":false,"start_time":"2025-06-08T21:03:26.661462","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Pivoting to stable legacy config: config/legacy_v0.x_configs/nanodet-m.yml\nLoading configuration file: config/nanodet_hituav.yml\n\n✅ Successfully modified and saved config/nanodet_hituav.yml. Ready for training!\n","output_type":"stream"}],"execution_count":21},{"id":"2a18e032","cell_type":"code","source":"!pip uninstall -y pytorch-lightning # Uninstall current version\n!pip install pytorch-lightning==1.9.5 # Install a compatible version","metadata":{"execution":{"iopub.status.busy":"2025-06-13T04:45:48.341525Z","iopub.execute_input":"2025-06-13T04:45:48.341868Z","iopub.status.idle":"2025-06-13T04:45:53.252235Z","shell.execute_reply.started":"2025-06-13T04:45:48.341823Z","shell.execute_reply":"2025-06-13T04:45:53.251173Z"},"papermill":{"duration":5.422531,"end_time":"2025-06-08T21:03:32.308691","exception":false,"start_time":"2025-06-08T21:03:26.886160","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Found existing installation: pytorch-lightning 1.9.5\nUninstalling pytorch-lightning-1.9.5:\n  Successfully uninstalled pytorch-lightning-1.9.5\nCollecting pytorch-lightning==1.9.5\n  Using cached pytorch_lightning-1.9.5-py3-none-any.whl.metadata (23 kB)\nRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning==1.9.5) (1.26.4)\nRequirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning==1.9.5) (2.6.0+cu124)\nRequirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning==1.9.5) (4.67.1)\nRequirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning==1.9.5) (6.0.2)\nRequirement already satisfied: fsspec>2021.06.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning==1.9.5) (2025.3.2)\nRequirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning==1.9.5) (1.7.1)\nRequirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning==1.9.5) (25.0)\nRequirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning==1.9.5) (4.13.2)\nRequirement already satisfied: lightning-utilities>=0.6.0.post0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning==1.9.5) (0.14.3)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning==1.9.5) (3.11.18)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.6.0.post0->pytorch-lightning==1.9.5) (75.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.2->pytorch-lightning==1.9.5) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.2->pytorch-lightning==1.9.5) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.2->pytorch-lightning==1.9.5) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.2->pytorch-lightning==1.9.5) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.2->pytorch-lightning==1.9.5) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.2->pytorch-lightning==1.9.5) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->pytorch-lightning==1.9.5) (3.18.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->pytorch-lightning==1.9.5) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->pytorch-lightning==1.9.5) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->pytorch-lightning==1.9.5) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->pytorch-lightning==1.9.5) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->pytorch-lightning==1.9.5) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->pytorch-lightning==1.9.5) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->pytorch-lightning==1.9.5) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->pytorch-lightning==1.9.5) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->pytorch-lightning==1.9.5) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->pytorch-lightning==1.9.5) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->pytorch-lightning==1.9.5) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->pytorch-lightning==1.9.5) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->pytorch-lightning==1.9.5) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->pytorch-lightning==1.9.5) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->pytorch-lightning==1.9.5) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->pytorch-lightning==1.9.5) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->pytorch-lightning==1.9.5) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.10.0->pytorch-lightning==1.9.5) (1.3.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9.5) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9.5) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9.5) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9.5) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9.5) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9.5) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9.5) (1.20.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10.0->pytorch-lightning==1.9.5) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.2->pytorch-lightning==1.9.5) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.2->pytorch-lightning==1.9.5) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17.2->pytorch-lightning==1.9.5) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17.2->pytorch-lightning==1.9.5) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17.2->pytorch-lightning==1.9.5) (2024.2.0)\nRequirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9.5) (3.10)\nUsing cached pytorch_lightning-1.9.5-py3-none-any.whl (829 kB)\nInstalling collected packages: pytorch-lightning\nSuccessfully installed pytorch-lightning-1.9.5\n","output_type":"stream"}],"execution_count":22},{"id":"1465e455","cell_type":"code","source":"# --- Clear CUDA cache before training attempt ---\n# This step is important to free up any residual GPU memory from previous operations\n# (like dataset conversion or previous failed runs). Clearing the cache reduces the\n# chance of Out-Of-Memory (OOM) errors during the training process, which can be\n# memory-intensive.\nprint(\"\\nClearing CUDA cache before starting training...\")\n# Check if a CUDA-compatible GPU is available.\nif torch.cuda.is_available():\n    torch.cuda.empty_cache() # Clears the PyTorch CUDA memory cache.\n    # Print the current GPU memory statistics to confirm the cache has been cleared.\n    # Values close to 0 GB indicate success.\n    print(f\"CUDA memory after clearing cache: {torch.cuda.memory_allocated()/1024**3:.2f} GB allocated, {torch.cuda.memory_reserved()/1024**3:.2f} GB reserved\")\nelse:\n    print(\"CUDA not available. Training will likely be very slow on CPU.\")\n\n# --- 3. Training the Model ---\n# This command initiates the actual training process of the NanoDet model.\n\nprint(\"\\n--- Starting Model Training ---\")\n# Execute the 'train.py' script, which is located in the 'tools/' directory of the NanoDet repository.\n# The path to our `custom_config_path` is passed as the main argument. This tells the training script\n# which configuration (dataset paths, model parameters, training schedule) to use.\n# The training progress (loss, mAP on validation set, etc.) will be printed to the standard output.\n!python tools/train.py {custom_config_path}\nprint(\"\\n✅ Model training command executed. Check logs for actual training progress.\")","metadata":{"execution":{"iopub.status.busy":"2025-06-13T04:45:57.536255Z","iopub.execute_input":"2025-06-13T04:45:57.537042Z","iopub.status.idle":"2025-06-13T04:52:21.012065Z","shell.execute_reply.started":"2025-06-13T04:45:57.537009Z","shell.execute_reply":"2025-06-13T04:52:21.010967Z"},"papermill":{"duration":3739.07819,"end_time":"2025-06-08T22:05:51.425554","exception":false,"start_time":"2025-06-08T21:03:32.347364","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"\nClearing CUDA cache before starting training...\nCUDA memory after clearing cache: 0.00 GB allocated, 0.00 GB reserved\n\n--- Starting Model Training ---\n\u001b[1m\u001b[35m[NanoDet]\u001b[0m\u001b[34m[06-13 04:46:04]\u001b[0m\u001b[32mINFO:\u001b[0m\u001b[97mSetting up data...\u001b[0m\nloading annotations into memory...\nDone (t=0.05s)\ncreating index...\nindex created!\nloading annotations into memory...\nDone (t=0.01s)\ncreating index...\nindex created!\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n\u001b[1m\u001b[35m[NanoDet]\u001b[0m\u001b[34m[06-13 04:46:04]\u001b[0m\u001b[32mINFO:\u001b[0m\u001b[97mCreating model...\u001b[0m\nmodel size is  1.0x\ninit weights...\n=> loading pretrained model https://download.pytorch.org/models/shufflenetv2_x1-5666bf0f80.pth\nFinish initialize NanoDet Head.\n/usr/local/lib/python3.11/dist-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n2025-06-13 04:46:15.094037: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749789975.118867     272 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749789975.127879     272 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\u001b[1m\u001b[35m[NanoDet]\u001b[0m\u001b[34m[06-13 04:46:18]\u001b[0m\u001b[32mINFO:\u001b[0m\u001b[97mTrain|Epoch1/20|Iter0(1/10)| mem:7.75G| lr:1.40e-02| loss_qfl:0.1187| loss_bbox:1.6215| loss_dfl:0.5239| \u001b[0m\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n\u001b[1m\u001b[35m[NanoDet]\u001b[0m\u001b[34m[06-13 04:46:38]\u001b[0m\u001b[32mINFO:\u001b[0m\u001b[97mTrain|Epoch2/20|Iter10(1/10)| mem:9.93G| lr:1.82e-02| loss_qfl:0.2123| loss_bbox:1.3557| loss_dfl:0.3533| \u001b[0m\n\u001b[1m\u001b[35m[NanoDet]\u001b[0m\u001b[34m[06-13 04:46:55]\u001b[0m\u001b[32mINFO:\u001b[0m\u001b[97mTrain|Epoch3/20|Iter20(1/10)| mem:9.93G| lr:2.24e-02| loss_qfl:0.2059| loss_bbox:1.0828| loss_dfl:0.2990| \u001b[0m\n\u001b[1m\u001b[35m[NanoDet]\u001b[0m\u001b[34m[06-13 04:47:13]\u001b[0m\u001b[32mINFO:\u001b[0m\u001b[97mTrain|Epoch4/20|Iter30(1/10)| mem:9.93G| lr:2.66e-02| loss_qfl:0.2270| loss_bbox:0.9262| loss_dfl:0.2635| \u001b[0m\n\u001b[1m\u001b[35m[NanoDet]\u001b[0m\u001b[34m[06-13 04:47:30]\u001b[0m\u001b[32mINFO:\u001b[0m\u001b[97mTrain|Epoch5/20|Iter40(1/10)| mem:9.93G| lr:3.08e-02| loss_qfl:0.2314| loss_bbox:0.8999| loss_dfl:0.2571| \u001b[0m\n\u001b[1m\u001b[35m[NanoDet]\u001b[0m\u001b[34m[06-13 04:47:48]\u001b[0m\u001b[32mINFO:\u001b[0m\u001b[97mTrain|Epoch6/20|Iter50(1/10)| mem:9.93G| lr:3.50e-02| loss_qfl:0.2156| loss_bbox:0.8258| loss_dfl:0.2451| \u001b[0m\n\u001b[1m\u001b[35m[NanoDet]\u001b[0m\u001b[34m[06-13 04:48:05]\u001b[0m\u001b[32mINFO:\u001b[0m\u001b[97mTrain|Epoch7/20|Iter60(1/10)| mem:9.93G| lr:3.92e-02| loss_qfl:0.1959| loss_bbox:0.8088| loss_dfl:0.2343| \u001b[0m\n\u001b[1m\u001b[35m[NanoDet]\u001b[0m\u001b[34m[06-13 04:48:22]\u001b[0m\u001b[32mINFO:\u001b[0m\u001b[97mTrain|Epoch8/20|Iter70(1/10)| mem:9.93G| lr:4.34e-02| loss_qfl:0.2237| loss_bbox:0.7510| loss_dfl:0.2343| \u001b[0m\n\u001b[1m\u001b[35m[NanoDet]\u001b[0m\u001b[34m[06-13 04:48:39]\u001b[0m\u001b[32mINFO:\u001b[0m\u001b[97mTrain|Epoch9/20|Iter80(1/10)| mem:9.93G| lr:4.76e-02| loss_qfl:0.2249| loss_bbox:0.7664| loss_dfl:0.2206| \u001b[0m\n\u001b[1m\u001b[35m[NanoDet]\u001b[0m\u001b[34m[06-13 04:48:56]\u001b[0m\u001b[32mINFO:\u001b[0m\u001b[97mTrain|Epoch10/20|Iter90(1/10)| mem:9.93G| lr:5.18e-02| loss_qfl:0.1844| loss_bbox:0.7211| loss_dfl:0.2222| \u001b[0m\n\u001b[1m\u001b[35m[NanoDet]\u001b[0m\u001b[34m[06-13 04:49:08]\u001b[0m\u001b[32mINFO:\u001b[0m\u001b[97mVal|Epoch10/20|Iter100(1/2)| mem:9.93G| lr:5.56e-02| loss_qfl:0.2429| loss_bbox:0.6950| loss_dfl:0.2013| \u001b[0m\nLoading and preparing results...\nDONE (t=0.12s)\ncreating index...\nindex created!\nRunning per image evaluation...\nEvaluate annotation type *bbox*\nDONE (t=2.67s).\nAccumulating evaluation results...\nDONE (t=0.24s).\n\u001b[1m\u001b[35m[NanoDet]\u001b[0m\u001b[34m[06-13 04:49:14]\u001b[0m\u001b[32mINFO:\u001b[0m\u001b[97m\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.098\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.263\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.053\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.061\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.119\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.122\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.056\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.181\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.232\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.140\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.332\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.180\n\u001b[0m\n\u001b[1m\u001b[35m[NanoDet]\u001b[0m\u001b[34m[06-13 04:49:14]\u001b[0m\u001b[32mINFO:\u001b[0m\u001b[97m\n| class   | AP50   | mAP   | class   | AP50   | mAP   |\n|:--------|:-------|:------|:--------|:-------|:------|\n| 0       | 34.7   | 9.8   | 1       | 66.7   | 29.5  |\n| 2       | 29.3   | 9.0   | 3       | 0.9    | 0.5   |\n| 4       | 0.0    | 0.0   |         |        |       |\u001b[0m\n\u001b[1m\u001b[35m[NanoDet]\u001b[0m\u001b[34m[06-13 04:49:14]\u001b[0m\u001b[32mINFO:\u001b[0m\u001b[97mSaving model to /kaggle/working/model_workspace/model_best/nanodet_model_best.pth\u001b[0m\n\u001b[1m\u001b[35m[NanoDet]\u001b[0m\u001b[34m[06-13 04:49:14]\u001b[0m\u001b[32mINFO:\u001b[0m\u001b[97mVal_metrics: {'mAP': 0.09754245231828355, 'AP_50': 0.263285158466436, 'AP_75': 0.05343090997274278, 'AP_small': 0.06093958192114596, 'AP_m': 0.11933290443256789, 'AP_l': 0.1224723208200994}\u001b[0m\n\u001b[1m\u001b[35m[NanoDet]\u001b[0m\u001b[34m[06-13 04:49:23]\u001b[0m\u001b[32mINFO:\u001b[0m\u001b[97mTrain|Epoch11/20|Iter100(1/10)| mem:8.14G| lr:5.60e-02| loss_qfl:0.1967| loss_bbox:0.6966| loss_dfl:0.2116| \u001b[0m\n\u001b[1m\u001b[35m[NanoDet]\u001b[0m\u001b[34m[06-13 04:49:40]\u001b[0m\u001b[32mINFO:\u001b[0m\u001b[97mTrain|Epoch12/20|Iter110(1/10)| mem:9.93G| lr:6.02e-02| loss_qfl:0.1851| loss_bbox:0.7224| loss_dfl:0.2215| \u001b[0m\n\u001b[1m\u001b[35m[NanoDet]\u001b[0m\u001b[34m[06-13 04:49:58]\u001b[0m\u001b[32mINFO:\u001b[0m\u001b[97mTrain|Epoch13/20|Iter120(1/10)| mem:9.93G| lr:6.44e-02| loss_qfl:0.1967| loss_bbox:0.6834| loss_dfl:0.2097| \u001b[0m\n\u001b[1m\u001b[35m[NanoDet]\u001b[0m\u001b[34m[06-13 04:50:15]\u001b[0m\u001b[32mINFO:\u001b[0m\u001b[97mTrain|Epoch14/20|Iter130(1/10)| mem:9.93G| lr:6.86e-02| loss_qfl:0.2495| loss_bbox:0.7021| loss_dfl:0.2049| \u001b[0m\n\u001b[1m\u001b[35m[NanoDet]\u001b[0m\u001b[34m[06-13 04:50:33]\u001b[0m\u001b[32mINFO:\u001b[0m\u001b[97mTrain|Epoch15/20|Iter140(1/10)| mem:9.93G| lr:7.28e-02| loss_qfl:0.2050| loss_bbox:0.6231| loss_dfl:0.2060| \u001b[0m\n\u001b[1m\u001b[35m[NanoDet]\u001b[0m\u001b[34m[06-13 04:50:50]\u001b[0m\u001b[32mINFO:\u001b[0m\u001b[97mTrain|Epoch16/20|Iter150(1/10)| mem:9.93G| lr:7.70e-02| loss_qfl:0.2162| loss_bbox:0.6406| loss_dfl:0.2037| \u001b[0m\n\u001b[1m\u001b[35m[NanoDet]\u001b[0m\u001b[34m[06-13 04:51:07]\u001b[0m\u001b[32mINFO:\u001b[0m\u001b[97mTrain|Epoch17/20|Iter160(1/10)| mem:9.93G| lr:8.12e-02| loss_qfl:0.1981| loss_bbox:0.6882| loss_dfl:0.2099| \u001b[0m\n\u001b[1m\u001b[35m[NanoDet]\u001b[0m\u001b[34m[06-13 04:51:25]\u001b[0m\u001b[32mINFO:\u001b[0m\u001b[97mTrain|Epoch18/20|Iter170(1/10)| mem:9.93G| lr:8.54e-02| loss_qfl:0.1819| loss_bbox:0.6341| loss_dfl:0.2083| \u001b[0m\n\u001b[1m\u001b[35m[NanoDet]\u001b[0m\u001b[34m[06-13 04:51:42]\u001b[0m\u001b[32mINFO:\u001b[0m\u001b[97mTrain|Epoch19/20|Iter180(1/10)| mem:9.93G| lr:8.96e-02| loss_qfl:0.2074| loss_bbox:0.6174| loss_dfl:0.1961| \u001b[0m\n\u001b[1m\u001b[35m[NanoDet]\u001b[0m\u001b[34m[06-13 04:51:59]\u001b[0m\u001b[32mINFO:\u001b[0m\u001b[97mTrain|Epoch20/20|Iter190(1/10)| mem:9.93G| lr:9.38e-02| loss_qfl:0.1770| loss_bbox:0.6274| loss_dfl:0.2015| \u001b[0m\n\u001b[1m\u001b[35m[NanoDet]\u001b[0m\u001b[34m[06-13 04:52:11]\u001b[0m\u001b[32mINFO:\u001b[0m\u001b[97mVal|Epoch20/20|Iter200(1/2)| mem:9.93G| lr:9.76e-02| loss_qfl:0.2123| loss_bbox:0.6236| loss_dfl:0.1923| \u001b[0m\nLoading and preparing results...\nDONE (t=0.78s)\ncreating index...\nindex created!\nRunning per image evaluation...\nEvaluate annotation type *bbox*\nDONE (t=2.57s).\nAccumulating evaluation results...\nDONE (t=0.22s).\n\u001b[1m\u001b[35m[NanoDet]\u001b[0m\u001b[34m[06-13 04:52:16]\u001b[0m\u001b[32mINFO:\u001b[0m\u001b[97m\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.118\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.314\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.061\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.094\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.132\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.148\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.056\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.211\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.256\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.169\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.339\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.210\n\u001b[0m\n\u001b[1m\u001b[35m[NanoDet]\u001b[0m\u001b[34m[06-13 04:52:16]\u001b[0m\u001b[32mINFO:\u001b[0m\u001b[97m\n| class   | AP50   | mAP   | class   | AP50   | mAP   |\n|:--------|:-------|:------|:--------|:-------|:------|\n| 0       | 39.5   | 11.2  | 1       | 77.5   | 35.4  |\n| 2       | 38.7   | 12.1  | 3       | 1.2    | 0.6   |\n| 4       | 0.1    | 0.0   |         |        |       |\u001b[0m\n\u001b[1m\u001b[35m[NanoDet]\u001b[0m\u001b[34m[06-13 04:52:16]\u001b[0m\u001b[32mINFO:\u001b[0m\u001b[97mSaving model to /kaggle/working/model_workspace/model_best/nanodet_model_best.pth\u001b[0m\n\u001b[1m\u001b[35m[NanoDet]\u001b[0m\u001b[34m[06-13 04:52:16]\u001b[0m\u001b[32mINFO:\u001b[0m\u001b[97mVal_metrics: {'mAP': 0.11844843382105862, 'AP_50': 0.31389589260180334, 'AP_75': 0.0609446266013118, 'AP_small': 0.09447985988438183, 'AP_m': 0.13174952324297548, 'AP_l': 0.1477063218167579}\u001b[0m\n\n✅ Model training command executed. Check logs for actual training progress.\n","output_type":"stream"}],"execution_count":23},{"id":"0ada467e","cell_type":"code","source":"# --- 4. Testing the Model (Evaluation) ---\n# This section is dedicated to evaluating the performance of the trained model on the unseen test dataset.\n\n# Define paths for evaluation.\n# `model_checkpoint_dir`: The directory where the trained model checkpoints are saved by NanoDet.\n# By default, NanoDet often creates a 'model_best/' subdirectory within the 'save_dir' specified in the config.\nmodel_checkpoint_dir = '/kaggle/working/model_workspace/model_best/'\n# `model_checkpoint_name`: The expected filename of the best saved model checkpoint.\n# NanoDet typically saves its best performing model during validation as 'nanodet_model_best.pth'.\nmodel_checkpoint_name = 'nanodet_model_best.pth'\n# `model_checkpoint_path`: The full, absolute path to the best model checkpoint file.\nmodel_checkpoint_path = os.path.join(model_checkpoint_dir, model_checkpoint_name)\n\n# Define the path to the NanoDet testing (evaluation) script.\ntest_script_path = '/kaggle/working/nanodet/tools/test.py'\n\n# --- WAITING FOR MODEL CHECKPOINT TO APPEAR ---\n# Training a deep learning model can take a significant amount of time (minutes to hours).\n# The `model_best.pth` file is only created once training reaches a certain point and\n# a new best model is saved. This loop introduces a waiting mechanism to ensure the\n# checkpoint file exists before attempting to load and evaluate the model.\nmax_wait_time = 1800  # Maximum wait time in seconds (equivalent to 30 minutes).\n                      # This should be adjusted based on the expected training duration for 200 epochs.\ncheck_interval = 30   # How often (in seconds) to check if the checkpoint file has appeared.\nelapsed_time = 0      # A counter to track how much time has passed during the wait.\n\nprint(\"\\nWaiting for training to complete and save a checkpoint (monitoring /kaggle/working/model_workspace/model_best/nanodet_model_best.pth)...\")\n\n# This 'while' loop continuously checks for the existence of the model checkpoint file.\n# It continues as long as the file is NOT found AND the maximum allowed wait time has not been exceeded.\nwhile not os.path.exists(model_checkpoint_path) and elapsed_time < max_wait_time:\n    print(f\"Model not found yet. Waiting... ({elapsed_time}/{max_wait_time} seconds elapsed)\")\n    time.sleep(check_interval) # Pause the execution for `check_interval` seconds.\n    elapsed_time += check_interval # Increment the elapsed time.\n\n# --- NEW: Checkpoint Key Renaming Function ---\n# This function is crucial for addressing potential compatibility issues with PyTorch model checkpoints.\n# Sometimes, when models are saved (especially if using PyTorch Lightning or specific wrappers),\n# the keys in the `state_dict` (which holds the model's learned weights) might differ from what\n# the model loading mechanism expects. A common issue is the presence or absence of a 'model.' prefix.\ndef rename_checkpoint_keys(checkpoint_path):\n    \"\"\"\n    Loads a PyTorch checkpoint and renames keys within its 'state_dict'\n    to ensure compatibility with the NanoDet test script. Specifically,\n    it adds a 'model.' prefix to keys like 'backbone.', 'fpn.', 'head.'\n    if they are missing it, which is a common pattern when models are saved\n    by PyTorch Lightning's Trainer compared to a direct `model.state_dict()`.\n\n    Args:\n        checkpoint_path (str): The file path to the original PyTorch model checkpoint (.pth file).\n\n    Returns:\n        str: The file path to the new, patched checkpoint file. This patched file\n             should be used for evaluation.\n    \"\"\"\n    print(f\"Attempting to rename keys in: {checkpoint_path}\")\n    # Load the checkpoint. `map_location='cpu'` ensures that the checkpoint can be loaded\n    # regardless of whether a GPU is available or if it was saved on a different device.\n    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n    \n    # Check if the loaded checkpoint dictionary contains a 'state_dict' key.\n    # PyTorch Lightning often wraps the model's state_dict under this key.\n    if 'state_dict' in checkpoint:\n        state_dict = checkpoint['state_dict']\n        print(\"Found 'state_dict' in checkpoint. Processing its keys.\")\n    else:\n        # If 'state_dict' is not present, assume the checkpoint itself is the raw state_dict.\n        state_dict = checkpoint\n        print(\"No 'state_dict' key found. Assuming checkpoint is raw state_dict. Processing its keys.\")\n\n    new_state_dict = {}\n    found_mismatches = False # A flag to track if any keys were actually renamed during this process.\n    for k, v in state_dict.items():\n        # This condition handles cases where keys *already* have 'model.' but might not be expected.\n        # However, the common problem for NanoDet is the *absence* of 'model.', so this path is less critical.\n        if k.startswith('model.'):\n            # If the key starts with 'model.', remove that prefix. This is for generality,\n            # though often not the issue in this specific NanoDet context.\n            new_k = k[6:]\n        # This is the primary fix for the NanoDet compatibility issue.\n        # If a key starts with components typical of a detection model's architecture (like backbone, FPN, head)\n        # but does *not* have the 'model.' prefix (which NanoDet's internal loading might expect), add it.\n        elif k.startswith('backbone.') or k.startswith('fpn.') or k.startswith('head.'):\n            new_k = 'model.' + k # Prepend 'model.' to the key.\n            found_mismatches = True # Indicate that a key was renamed.\n        else:\n            # For all other keys (e.g., optimizer states, epoch info), keep them as they are.\n            new_k = k\n        new_state_dict[new_k] = v\n\n    if found_mismatches:\n        print(f\"✅ Successfully renamed keys by adding 'model.' prefix.\")\n        # Update the original checkpoint dictionary with the newly structured state_dict.\n        if 'state_dict' in checkpoint:\n            checkpoint['state_dict'] = new_state_dict\n        else:\n            checkpoint = new_state_dict\n    else:\n        print(\"No 'model.' prefix mismatches detected. Using original keys.\")\n        \n    # Save the modified checkpoint to a new file. It's safer to create a new patched file\n    # rather than overwriting the original, especially if debugging.\n    temp_model_path = model_checkpoint_path.replace(\".pth\", \"_patched.pth\")\n    torch.save(checkpoint, temp_model_path) # Save the modified checkpoint.\n    print(f\"Patched checkpoint saved to: {temp_model_path}\")\n    return temp_model_path # Return the path to the newly created patched checkpoint.","metadata":{"execution":{"iopub.status.busy":"2025-06-13T04:52:44.757192Z","iopub.execute_input":"2025-06-13T04:52:44.757902Z","iopub.status.idle":"2025-06-13T04:52:44.767052Z","shell.execute_reply.started":"2025-06-13T04:52:44.757878Z","shell.execute_reply":"2025-06-13T04:52:44.766295Z"},"papermill":{"duration":0.064852,"end_time":"2025-06-08T22:05:51.544982","exception":false,"start_time":"2025-06-08T22:05:51.480130","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"\nWaiting for training to complete and save a checkpoint (monitoring /kaggle/working/model_workspace/model_best/nanodet_model_best.pth)...\n","output_type":"stream"}],"execution_count":26},{"id":"8fef13b3","cell_type":"code","source":"# Continue from the waiting loop in the previous conceptual cell.\n# If the checkpoint file was not found within the maximum wait time:\nif not os.path.exists(model_checkpoint_path):\n    print(\"\\n🛑 Error: Training did not complete or model checkpoint was not found within the maximum wait time.\")\n    print(\"Testing cannot proceed.\")\n    # You could uncomment 'sys.exit(1)' to halt the notebook execution here if the model is not found,\n    # preventing subsequent errors.\n    # sys.exit(1)\nelse:\n    print(\"\\n✅ Training complete and model checkpoint found!\")\n    \n    # --- PROCEED WITH TESTING ---\n    print(f\"\\n--- Starting Model Evaluation on Test Set ---\")\n    print(f\"Evaluating model: {model_checkpoint_path}\")\n    \n    # Define the path to the test annotation file. This file was created during the COCO conversion step.\n    test_ann_path = '/kaggle/working/hituav_coco/annotations/test.json'\n    \n    # Call the `rename_checkpoint_keys` function to get a compatible model path.\n    # This function handles potential key mismatches in the saved PyTorch checkpoint.\n    model_to_test_path = rename_checkpoint_keys(model_checkpoint_path)\n\n    # Execute the `test.py` script from the NanoDet 'tools/' directory.\n    # The script takes:\n    # --config: Path to the custom configuration file (defines dataset, model architecture).\n    # --model: Path to the (potentially patched) trained model checkpoint.\n    # The evaluation results (mAP, precision, recall) will be printed to stdout.\n    !python {test_script_path} --config {custom_config_path} --model {model_to_test_path}\n    print(\"\\n✅ Model testing complete!\")","metadata":{"execution":{"iopub.status.busy":"2025-06-13T04:52:54.006355Z","iopub.execute_input":"2025-06-13T04:52:54.007086Z","iopub.status.idle":"2025-06-13T04:53:15.322798Z","shell.execute_reply.started":"2025-06-13T04:52:54.007061Z","shell.execute_reply":"2025-06-13T04:53:15.322016Z"},"papermill":{"duration":20.340734,"end_time":"2025-06-08T22:06:11.938708","exception":false,"start_time":"2025-06-08T22:05:51.597974","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"\n✅ Training complete and model checkpoint found!\n\n--- Starting Model Evaluation on Test Set ---\nEvaluating model: /kaggle/working/model_workspace/model_best/nanodet_model_best.pth\nAttempting to rename keys in: /kaggle/working/model_workspace/model_best/nanodet_model_best.pth\nFound 'state_dict' in checkpoint. Processing its keys.\n✅ Successfully renamed keys by adding 'model.' prefix.\nPatched checkpoint saved to: /kaggle/working/model_workspace/model_best/nanodet_model_best_patched.pth\n\u001b[1m\u001b[35m[NanoDet]\u001b[0m\u001b[34m[06-13 04:53:00]\u001b[0m\u001b[32mINFO:\u001b[0m\u001b[97mSetting up data...\u001b[0m\nloading annotations into memory...\nDone (t=0.01s)\ncreating index...\nindex created!\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n\u001b[1m\u001b[35m[NanoDet]\u001b[0m\u001b[34m[06-13 04:53:00]\u001b[0m\u001b[32mINFO:\u001b[0m\u001b[97mCreating model...\u001b[0m\nmodel size is  1.0x\ninit weights...\n=> loading pretrained model https://download.pytorch.org/models/shufflenetv2_x1-5666bf0f80.pth\nFinish initialize NanoDet Head.\n/kaggle/working/nanodet/tools/test.py:79: UserWarning: Warning! Old .pth checkpoint is deprecated. Convert the checkpoint with tools/convert_old_checkpoint.py \n  warnings.warn(\n\u001b[1m\u001b[35m[NanoDet]\u001b[0m\u001b[34m[06-13 04:53:01]\u001b[0m\u001b[32mINFO:\u001b[0m\u001b[97mStarting testing...\u001b[0m\nTesting DataLoader 0:   0%|                               | 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\nTesting DataLoader 0: 100%|███████████████████████| 2/2 [00:01<00:00,  1.06it/s]Loading and preparing results...\nDONE (t=0.35s)\ncreating index...\nindex created!\nRunning per image evaluation...\nEvaluate annotation type *bbox*\nDONE (t=2.48s).\nAccumulating evaluation results...\nDONE (t=0.23s).\n\u001b[1m\u001b[35m[NanoDet]\u001b[0m\u001b[34m[06-13 04:53:09]\u001b[0m\u001b[32mINFO:\u001b[0m\u001b[97m\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.118\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.314\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.061\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.094\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.132\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.148\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.056\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.211\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.256\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.169\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.339\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.210\n\u001b[0m\n\u001b[1m\u001b[35m[NanoDet]\u001b[0m\u001b[34m[06-13 04:53:09]\u001b[0m\u001b[32mINFO:\u001b[0m\u001b[97m\n| class   | AP50   | mAP   | class   | AP50   | mAP   |\n|:--------|:-------|:------|:--------|:-------|:------|\n| 0       | 39.5   | 11.2  | 1       | 77.5   | 35.4  |\n| 2       | 38.7   | 12.1  | 3       | 1.2    | 0.6   |\n| 4       | 0.1    | 0.0   |         |        |       |\u001b[0m\nTesting DataLoader 0: 100%|███████████████████████| 2/2 [00:06<00:00,  3.18s/it]\n2025-06-13 04:53:09.998044: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749790390.015732    1028 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749790390.020921    1028 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\n✅ Model testing complete!\n","output_type":"stream"}],"execution_count":27}]}